{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:lightrag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:lightrag:Load KV llm_response_cache with 0 data\n",
      "INFO:lightrag:Load KV full_docs with 1 data\n",
      "INFO:lightrag:Load KV text_chunks with 1729 data\n",
      "INFO:lightrag:Loaded graph from C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\graph_chunk_entity_relation.graphml with 37799 nodes, 15142 edges\n",
      "INFO:nano-vectordb:Load (36668, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_entities.json'} 36668 data\n",
      "INFO:nano-vectordb:Load (15142, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_relationships.json'} 15142 data\n",
      "INFO:nano-vectordb:Load (1729, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_chunks.json'} 1729 data\n",
      "INFO:lightrag:Loaded document status storage with 1 records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "\n",
    "#########\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "#########\n",
    "\n",
    "WORKING_DIR = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\"\n",
    "\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=gpt_4o_mini_complete  # Use gpt_4o_mini_complete LLM model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import tiktoken\n",
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string using tiktoken\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The model to use for tokenization (default: \"gpt-4\")\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base encoding if model not found\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Local query uses 15 entites, 2 relations, 1 text units\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.464971 seconds\n",
      "INFO:lightrag:Global query uses 30 entites, 15 relations, 1 text units\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story of \"Maggie, A Girl of the Streets\" by Stephen Crane revolves around several prominent themes that are central to its narrative. Here are the key themes present in the story:\n",
      "\n",
      "1. **Poverty and Social Realism**: One of the most significant themes is the impact of poverty on individuals and families. The narrative depicts the harsh realities faced by the characters living in the Bowery, highlighting how economic conditions can lead to desperation and tragic outcomes.\n",
      "\n",
      "2. **Isolation and Loneliness**: Maggie experiences profound loneliness stemming from her impoverished upbringing. The theme of solitude is intertwined with her relationships, or lack thereof, reflecting how her environment contributes to her isolation.\n",
      "\n",
      "3. **Gender and Femininity**: The story explores the challenges and societal expectations placed on women. Maggie's struggles in a male-dominated world illustrate the limited options available to women, particularly those from impoverished backgrounds.\n",
      "\n",
      "4. **The Influence of Environment**: Crane emphasizes how the environment shapes the characters' lives and choices. The Bowery, with its depiction of vice and despair, serves as a backdrop influencing Maggie's fate and decisions.\n",
      "\n",
      "5. **Desire and Disillusionment**: Maggie's aspirations for love and a better life are met with disappointment. Her longing for validation and escape contrasts tragically with her reality, highlighting the theme of disillusionment.\n",
      "\n",
      "6. **Human Struggle and Tragedy**: The story encapsulates the broader human struggles against societal constraints and personal demons. It emphasizes the tragic nature of many lives caught in the cycle of poverty and rejection.\n",
      "\n",
      "These themes work together to create a powerful critique of societal norms, as well as a poignant character study that reflects the complexities of human experience amid adversity. \"Maggie, A Girl of the Streets\" is a profound exploration of the human condition, marked by Crane's signature realism and psychological depth.\n",
      "5780\n"
     ]
    }
   ],
   "source": [
    "context,retrieval_context=rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "print(context)\n",
    "print(count_tokens(retrieval_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Out to Win is an American documentary film tha...</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>Are both Variety and The Advocate LGBT-interes...</td>\n",
       "      <td>no</td>\n",
       "      <td>comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Who is this American rapper, songwriter, recor...</td>\n",
       "      <td>Lil' Kim</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>In which year was this American country music ...</td>\n",
       "      <td>2000</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>What is the nationality of the actor who starr...</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question           answer  \\\n",
       "1116  Out to Win is an American documentary film tha...  Houston Rockets   \n",
       "1368  Are both Variety and The Advocate LGBT-interes...               no   \n",
       "422   Who is this American rapper, songwriter, recor...         Lil' Kim   \n",
       "413   In which year was this American country music ...             2000   \n",
       "451   What is the nationality of the actor who starr...         Scottish   \n",
       "\n",
       "            type  \n",
       "1116      bridge  \n",
       "1368  comparison  \n",
       "422       bridge  \n",
       "413       bridge  \n",
       "451       bridge  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "musique_df=pd.read_parquet(r\"D:\\github\\TGRAG_eval\\HotpotQA\\hotpotqa_question_answer_type_200.parquet\")\n",
    "musique_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/200 [00:00<?, ?it/s]INFO:openai._base_client:Retrying request to /embeddings in 0.464790 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.798884 seconds\n",
      "INFO:lightrag:Local query uses 15 entites, 14 relations, 1 text units\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.428835 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.940137 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.431933 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.927320 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.377045 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.997221 seconds\n",
      "Processing samples:   0%|          | 0/200 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x20e40cd6bc0 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\llm.py:904\u001b[0m, in \u001b[0;36mopenai_embedding\u001b[1;34m(texts, model, base_url, api_key)\u001b[0m\n\u001b[0;32m    901\u001b[0m openai_async_client \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    902\u001b[0m     AsyncOpenAI() \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m AsyncOpenAI(base_url\u001b[38;5;241m=\u001b[39mbase_url)\n\u001b[0;32m    903\u001b[0m )\n\u001b[1;32m--> 904\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai_async_client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    905\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtexts, encoding_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([dp\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata])\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\resources\\embeddings.py:236\u001b[0m, in \u001b[0;36mAsyncEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    238\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    239\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    240\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    241\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    242\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    243\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    244\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    245\u001b[0m     ),\n\u001b[0;32m    246\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1840\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1841\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1842\u001b[0m )\n\u001b[1;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1543\u001b[0m )\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\openai\\_base_client.py:1638\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1637\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1641\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1642\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1647\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start timing\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the answer from TGRAG_search\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m response, retrieved_context \u001b[38;5;241m=\u001b[39m \u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQueryParam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhybrid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mwith_retrieval_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate processing time\u001b[39;00m\n\u001b[0;32m     19\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\lightrag.py:886\u001b[0m, in \u001b[0;36mLightRAG.query\u001b[1;34m(self, query, param)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, param: QueryParam \u001b[38;5;241m=\u001b[39m QueryParam()):\n\u001b[0;32m    885\u001b[0m     loop \u001b[38;5;241m=\u001b[39m always_get_an_event_loop()\n\u001b[1;32m--> 886\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\lightrag.py:890\u001b[0m, in \u001b[0;36mLightRAG.aquery\u001b[1;34m(self, query, param)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, param: QueryParam \u001b[38;5;241m=\u001b[39m QueryParam()):\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 890\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kg_query(\n\u001b[0;32m    891\u001b[0m             query,\n\u001b[0;32m    892\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_entity_relation_graph,\n\u001b[0;32m    893\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentities_vdb,\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelationships_vdb,\n\u001b[0;32m    895\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_chunks,\n\u001b[0;32m    896\u001b[0m             param,\n\u001b[0;32m    897\u001b[0m             asdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    898\u001b[0m             hashing_kv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_response_cache\n\u001b[0;32m    899\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_response_cache\n\u001b[0;32m    900\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_response_cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    901\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_string_value_json_storage_cls(\n\u001b[0;32m    902\u001b[0m                 namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_response_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    903\u001b[0m                 global_config\u001b[38;5;241m=\u001b[39masdict(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    904\u001b[0m                 embedding_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    905\u001b[0m             ),\n\u001b[0;32m    906\u001b[0m         )\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    908\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m naive_query(\n\u001b[0;32m    909\u001b[0m             query,\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks_vdb,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    921\u001b[0m             ),\n\u001b[0;32m    922\u001b[0m         )\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\operate.py:646\u001b[0m, in \u001b[0;36mkg_query\u001b[1;34m(query, knowledge_graph_inst, entities_vdb, relationships_vdb, text_chunks_db, query_param, global_config, hashing_kv)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# logger.info(\"Using %s mode for query processing\", query_param.mode)\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Build context\u001b[39;00m\n\u001b[0;32m    645\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [ll_keywords, hl_keywords]\n\u001b[1;32m--> 646\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _build_query_context(\n\u001b[0;32m    647\u001b[0m     keywords,\n\u001b[0;32m    648\u001b[0m     knowledge_graph_inst,\n\u001b[0;32m    649\u001b[0m     entities_vdb,\n\u001b[0;32m    650\u001b[0m     relationships_vdb,\n\u001b[0;32m    651\u001b[0m     text_chunks_db,\n\u001b[0;32m    652\u001b[0m     query_param,\n\u001b[0;32m    653\u001b[0m )\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# if query_param.only_need_context:\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;66;03m#     return context\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\operate.py:954\u001b[0m, in \u001b[0;36m_build_query_context\u001b[1;34m(query, knowledge_graph_inst, entities_vdb, relationships_vdb, text_chunks_db, query_param)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# hybrid mode\u001b[39;00m\n\u001b[0;32m    939\u001b[0m         (\n\u001b[0;32m    940\u001b[0m             ll_entities_context,\n\u001b[0;32m    941\u001b[0m             ll_relations_context,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m             query_param,\n\u001b[0;32m    949\u001b[0m         )\n\u001b[0;32m    950\u001b[0m         (\n\u001b[0;32m    951\u001b[0m             hl_entities_context,\n\u001b[0;32m    952\u001b[0m             hl_relations_context,\n\u001b[0;32m    953\u001b[0m             hl_text_units_context,\n\u001b[1;32m--> 954\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _get_edge_data(\n\u001b[0;32m    955\u001b[0m             hl_keywords,\n\u001b[0;32m    956\u001b[0m             knowledge_graph_inst,\n\u001b[0;32m    957\u001b[0m             relationships_vdb,\n\u001b[0;32m    958\u001b[0m             text_chunks_db,\n\u001b[0;32m    959\u001b[0m             query_param,\n\u001b[0;32m    960\u001b[0m         )\n\u001b[0;32m    961\u001b[0m         entities_context, relations_context, text_units_context \u001b[38;5;241m=\u001b[39m combine_contexts(\n\u001b[0;32m    962\u001b[0m             [hl_entities_context, ll_entities_context],\n\u001b[0;32m    963\u001b[0m             [hl_relations_context, ll_relations_context],\n\u001b[0;32m    964\u001b[0m             [hl_text_units_context, ll_text_units_context],\n\u001b[0;32m    965\u001b[0m         )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124m-----Entities-----\u001b[39m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124m```csv\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;124m```\u001b[39m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\operate.py:1194\u001b[0m, in \u001b[0;36m_get_edge_data\u001b[1;34m(keywords, knowledge_graph_inst, relationships_vdb, text_chunks_db, query_param)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_edge_data\u001b[39m(\n\u001b[0;32m   1188\u001b[0m     keywords,\n\u001b[0;32m   1189\u001b[0m     knowledge_graph_inst: BaseGraphStorage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     query_param: QueryParam,\n\u001b[0;32m   1193\u001b[0m ):\n\u001b[1;32m-> 1194\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m relationships_vdb\u001b[38;5;241m.\u001b[39mquery(keywords, top_k\u001b[38;5;241m=\u001b[39mquery_param\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results):\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\storage.py:161\u001b[0m, in \u001b[0;36mNanoVectorDBStorage.query\u001b[1;34m(self, query, top_k)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m--> 161\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_func([query])\n\u001b[0;32m    162\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embedding[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    163\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mquery(\n\u001b[0;32m    164\u001b[0m         query\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    165\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    166\u001b[0m         better_than_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosine_better_than_threshold,\n\u001b[0;32m    167\u001b[0m     )\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\utils.py:132\u001b[0m, in \u001b[0;36mlimit_async_func_call.<locals>.final_decro.<locals>.wait_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(waitting_time)\n\u001b[0;32m    131\u001b[0m __current_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m __current_size \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\github\\LightRAG\\lightrag\\utils.py:71\u001b[0m, in \u001b[0;36mEmbeddingFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semaphore:\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    188\u001b[0m async_wrapped\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tenacity\\__init__.py:419\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x20e40cd6bc0 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store retrieved contexts and responses\n",
    "retrieved_contexts = []\n",
    "responses = []\n",
    "processing_time = []\n",
    "retrieval_tokens = []\n",
    "\n",
    "# Iterate over the first 175 samples of the musique_data dataframe\n",
    "for index in tqdm(range(len(musique_df)), desc=\"Processing samples\"):\n",
    "    question = musique_df.iloc[index]['question']  # Get the question from the dataframe\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Get the answer from TGRAG_search\n",
    "    response, retrieved_context = rag.query(question, param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "\n",
    "    # Calculate processing time\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Append the retrieved context and response to the lists\n",
    "    retrieved_contexts.append(retrieved_context)\n",
    "    responses.append(response)\n",
    "    processing_time.append(elapsed_time)\n",
    "    retrieval_tokens.append(count_tokens(retrieved_context))\n",
    "\n",
    "# Add the retrieved contexts and responses to the musique_data dataframe\n",
    "musique_df['retrieved_context'] = retrieved_contexts\n",
    "musique_df['response'] = responses\n",
    "musique_df['processing_time'] = processing_time\n",
    "musique_df['retrieval_tokens'] = retrieval_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\output\\LightRAG_hotpotqa_200sample_responses2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average processing time: 5.58 seconds\n"
     ]
    }
   ],
   "source": [
    "average_processing_time = musique_df['processing_time'].mean()\n",
    "print(f\"Average processing time: {average_processing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluations: 100%|██████████| 200/200 [07:58<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm  # Ensure tqdm is imported\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def get_gpt4_response(answer, response):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please evaluate if the response matches the reference answer.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Instructions\\nYou will receive a ground truth answer (referred to as Answer) and a model-generated answer (referred to as Response). Your task is to compare the two and determine whether they align.\\n\\nNote: The ground truth answer may sometimes be embedded within the model-generated answer. You need to carefully analyze and discern whether they align.\\nYour Output:\\nIf the two answers align, respond with yes.\\nIf they do not align, respond with no.\\nIf you are very uncertain, respond with unclear.\\nYour response should first include yes, no, or unclear, followed by an explanation.\\n\\nExample 1\\nAnswer: Houston Rockets\\nResponse: The basketball player who was drafted 18th overall in 2001 is Jason Collins, who was selected by the Houston Rockets.\\nExpeted output: yes\\n\\nExample 2\\nAnswer: no\\nResponse: Yes, both Variety and The Advocate are LGBT-interest magazines. The Advocate is explicitly identified as an American LGBT-interest magazine, while Variety, although primarily known for its coverage of the entertainment industry, also addresses topics relevant to the LGBT community.\\n Expected output: no\\n\\nInput Data Format\\nGround Truth Answer: {answer}\\nModel Generated Answer: {response}\\n\\nExpected Output\\nyes, no, or unclear\\nAn explanation of your choice.\\n\\nOutput:\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting GPT-4 response: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "# Wrap the loop with tqdm to monitor progress\n",
    "for idx, row in tqdm(musique_df.iterrows(), total=len(musique_df), desc=\"Processing evaluations\"):\n",
    "    \n",
    "    # Get evaluation from GPT-4\n",
    "    evaluation = get_gpt4_response(row['answer'], row['response'])\n",
    "    \n",
    "    # Update dataframe with evaluation\n",
    "    musique_df.at[idx, 'gpt4_evaluation'] = evaluation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of evaluations starting with 'yes': 79.00%\n"
     ]
    }
   ],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\output\\LightRAG_hotpotqa_200sample_responses_with_gpt4_evaluation2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)\n",
    "musique_df['evaluation_result'] = musique_df['gpt4_evaluation'].apply(lambda x: 1 if 'yes' in x.lower() else 0)\n",
    "\n",
    "# Calculate the percentage of 1s\n",
    "percentage_yes = (musique_df['evaluation_result'].sum() / len(musique_df)) * 100\n",
    "\n",
    "print(f\"Percentage of evaluations starting with 'yes': {percentage_yes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      200.000000\n",
       "mean      7176.735000\n",
       "std        572.546006\n",
       "min       5390.000000\n",
       "25%       6945.750000\n",
       "50%       7197.500000\n",
       "75%       7495.500000\n",
       "max      10087.000000\n",
       "Name: retrieval_tokens, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musique_df['retrieval_tokens'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
