{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\lightrag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:lightrag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:lightrag:Load KV llm_response_cache with 0 data\n",
      "INFO:lightrag:Load KV full_docs with 1 data\n",
      "INFO:lightrag:Load KV text_chunks with 1729 data\n",
      "INFO:lightrag:Loaded graph from C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\graph_chunk_entity_relation.graphml with 37799 nodes, 15142 edges\n",
      "INFO:nano-vectordb:Load (36668, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_entities.json'} 36668 data\n",
      "INFO:nano-vectordb:Load (15142, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_relationships.json'} 15142 data\n",
      "INFO:nano-vectordb:Load (1729, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_HotpotQA\\\\vdb_chunks.json'} 1729 data\n",
      "INFO:lightrag:Loaded document status storage with 1 records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "\n",
    "#########\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "#########\n",
    "\n",
    "WORKING_DIR = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\"\n",
    "\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=gpt_4o_mini_complete  # Use gpt_4o_mini_complete LLM model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import tiktoken\n",
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string using tiktoken\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The model to use for tokenization (default: \"gpt-4\")\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base encoding if model not found\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Local query uses 15 entites, 2 relations, 1 text units\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.464971 seconds\n",
      "INFO:lightrag:Global query uses 30 entites, 15 relations, 1 text units\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story of \"Maggie, A Girl of the Streets\" by Stephen Crane revolves around several prominent themes that are central to its narrative. Here are the key themes present in the story:\n",
      "\n",
      "1. **Poverty and Social Realism**: One of the most significant themes is the impact of poverty on individuals and families. The narrative depicts the harsh realities faced by the characters living in the Bowery, highlighting how economic conditions can lead to desperation and tragic outcomes.\n",
      "\n",
      "2. **Isolation and Loneliness**: Maggie experiences profound loneliness stemming from her impoverished upbringing. The theme of solitude is intertwined with her relationships, or lack thereof, reflecting how her environment contributes to her isolation.\n",
      "\n",
      "3. **Gender and Femininity**: The story explores the challenges and societal expectations placed on women. Maggie's struggles in a male-dominated world illustrate the limited options available to women, particularly those from impoverished backgrounds.\n",
      "\n",
      "4. **The Influence of Environment**: Crane emphasizes how the environment shapes the characters' lives and choices. The Bowery, with its depiction of vice and despair, serves as a backdrop influencing Maggie's fate and decisions.\n",
      "\n",
      "5. **Desire and Disillusionment**: Maggie's aspirations for love and a better life are met with disappointment. Her longing for validation and escape contrasts tragically with her reality, highlighting the theme of disillusionment.\n",
      "\n",
      "6. **Human Struggle and Tragedy**: The story encapsulates the broader human struggles against societal constraints and personal demons. It emphasizes the tragic nature of many lives caught in the cycle of poverty and rejection.\n",
      "\n",
      "These themes work together to create a powerful critique of societal norms, as well as a poignant character study that reflects the complexities of human experience amid adversity. \"Maggie, A Girl of the Streets\" is a profound exploration of the human condition, marked by Crane's signature realism and psychological depth.\n",
      "5780\n"
     ]
    }
   ],
   "source": [
    "# context,retrieval_context=rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "# print(context)\n",
    "# print(count_tokens(retrieval_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>Out to Win is an American documentary film tha...</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>Are both Variety and The Advocate LGBT-interes...</td>\n",
       "      <td>no</td>\n",
       "      <td>comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Who is this American rapper, songwriter, recor...</td>\n",
       "      <td>Lil' Kim</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>In which year was this American country music ...</td>\n",
       "      <td>2000</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>What is the nationality of the actor who starr...</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question           answer  \\\n",
       "1116  Out to Win is an American documentary film tha...  Houston Rockets   \n",
       "1368  Are both Variety and The Advocate LGBT-interes...               no   \n",
       "422   Who is this American rapper, songwriter, recor...         Lil' Kim   \n",
       "413   In which year was this American country music ...             2000   \n",
       "451   What is the nationality of the actor who starr...         Scottish   \n",
       "\n",
       "            type  \n",
       "1116      bridge  \n",
       "1368  comparison  \n",
       "422       bridge  \n",
       "413       bridge  \n",
       "451       bridge  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "musique_df=pd.read_parquet(r\"D:\\github\\TGRAG_eval\\HotpotQA\\hotpotqa_question_answer_type_200.parquet\")\n",
    "musique_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store retrieved contexts and responses\n",
    "retrieved_contexts = []\n",
    "responses = []\n",
    "processing_time = []\n",
    "retrieval_tokens = []\n",
    "\n",
    "# Iterate over the first 175 samples of the musique_data dataframe\n",
    "for index in tqdm(range(len(musique_df)), desc=\"Processing samples\"):\n",
    "    question = musique_df.iloc[index]['question']  # Get the question from the dataframe\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Get the answer from TGRAG_search\n",
    "    response, retrieved_context = rag.query(question, param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "\n",
    "    # Calculate processing time\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Append the retrieved context and response to the lists\n",
    "    retrieved_contexts.append(retrieved_context)\n",
    "    responses.append(response)\n",
    "    processing_time.append(elapsed_time)\n",
    "    retrieval_tokens.append(count_tokens(retrieved_context))\n",
    "\n",
    "# Add the retrieved contexts and responses to the musique_data dataframe\n",
    "musique_df['retrieved_context'] = retrieved_contexts\n",
    "musique_df['response'] = responses\n",
    "musique_df['processing_time'] = processing_time\n",
    "musique_df['retrieval_tokens'] = retrieval_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\output\\LightRAG_hotpotqa_200sample_responses2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average processing time: 5.58 seconds\n"
     ]
    }
   ],
   "source": [
    "average_processing_time = musique_df['processing_time'].mean()\n",
    "print(f\"Average processing time: {average_processing_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluations: 100%|██████████| 200/200 [07:58<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm  # Ensure tqdm is imported\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def get_gpt4_response(answer, response):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please evaluate if the response matches the reference answer.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Instructions\\nYou will receive a ground truth answer (referred to as Answer) and a model-generated answer (referred to as Response). Your task is to compare the two and determine whether they align.\\n\\nNote: The ground truth answer may sometimes be embedded within the model-generated answer. You need to carefully analyze and discern whether they align.\\nYour Output:\\nIf the two answers align, respond with yes.\\nIf they do not align, respond with no.\\nIf you are very uncertain, respond with unclear.\\nYour response should first include yes, no, or unclear, followed by an explanation.\\n\\nExample 1\\nAnswer: Houston Rockets\\nResponse: The basketball player who was drafted 18th overall in 2001 is Jason Collins, who was selected by the Houston Rockets.\\nExpeted output: yes\\n\\nExample 2\\nAnswer: no\\nResponse: Yes, both Variety and The Advocate are LGBT-interest magazines. The Advocate is explicitly identified as an American LGBT-interest magazine, while Variety, although primarily known for its coverage of the entertainment industry, also addresses topics relevant to the LGBT community.\\n Expected output: no\\n\\nInput Data Format\\nGround Truth Answer: {answer}\\nModel Generated Answer: {response}\\n\\nExpected Output\\nyes, no, or unclear\\nAn explanation of your choice.\\n\\nOutput:\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting GPT-4 response: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "# Wrap the loop with tqdm to monitor progress\n",
    "for idx, row in tqdm(musique_df.iterrows(), total=len(musique_df), desc=\"Processing evaluations\"):\n",
    "    \n",
    "    # Get evaluation from GPT-4\n",
    "    evaluation = get_gpt4_response(row['answer'], row['response'])\n",
    "    \n",
    "    # Update dataframe with evaluation\n",
    "    musique_df.at[idx, 'gpt4_evaluation'] = evaluation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of evaluations starting with 'yes': 79.00%\n"
     ]
    }
   ],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_HotpotQA\\output\\LightRAG_hotpotqa_200sample_responses_with_gpt4_evaluation2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)\n",
    "musique_df['evaluation_result'] = musique_df['gpt4_evaluation'].apply(lambda x: 1 if 'yes' in x.lower() else 0)\n",
    "\n",
    "# Calculate the percentage of 1s\n",
    "percentage_yes = (musique_df['evaluation_result'].sum() / len(musique_df)) * 100\n",
    "\n",
    "print(f\"Percentage of evaluations starting with 'yes': {percentage_yes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      200.000000\n",
       "mean      7176.735000\n",
       "std        572.546006\n",
       "min       5390.000000\n",
       "25%       6945.750000\n",
       "50%       7197.500000\n",
       "75%       7495.500000\n",
       "max      10087.000000\n",
       "Name: retrieval_tokens, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musique_df['retrieval_tokens'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
