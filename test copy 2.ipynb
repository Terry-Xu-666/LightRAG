{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: C:\\Users\\Terry_Xu\\Desktop\\LightRAG_MultiHopRAG\n",
      "INFO:lightrag:Load KV json_doc_status_storage with 0 data\n",
      "INFO:lightrag:Load KV llm_response_cache with 0 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Load KV full_docs with 1 data\n",
      "INFO:lightrag:Load KV text_chunks with 1266 data\n",
      "INFO:lightrag:Loaded graph from C:\\Users\\Terry_Xu\\Desktop\\LightRAG_MultiHopRAG\\graph_chunk_entity_relation.graphml with 19481 nodes, 11123 edges\n",
      "INFO:nano-vectordb:Load (19237, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_MultiHopRAG\\\\vdb_entities.json'} 19237 data\n",
      "INFO:nano-vectordb:Load (11123, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_MultiHopRAG\\\\vdb_relationships.json'} 11123 data\n",
      "INFO:nano-vectordb:Load (1266, 1536) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1536, 'metric': 'cosine', 'storage_file': 'C:\\\\Users\\\\Terry_Xu\\\\Desktop\\\\LightRAG_MultiHopRAG\\\\vdb_chunks.json'} 1266 data\n",
      "INFO:lightrag:Loaded document status storage with 1 records\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\n",
    "\n",
    "#########\n",
    "# Uncomment the below two lines if running in a jupyter notebook to handle the async nature of rag.insert()\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "#########\n",
    "\n",
    "WORKING_DIR = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_MultiHopRAG\"\n",
    "\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=gpt_4o_mini_complete  # Use gpt_4o_mini_complete LLM model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import tiktoken\n",
    "def count_tokens(text: str, model: str = \"gpt-4\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string using tiktoken\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The model to use for tokenization (default: \"gpt-4\")\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base encoding if model not found\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:kw_prompt result:\n",
      "INFO:lightrag:Using hybrid mode for query processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"high_level_keywords\": [\"Top themes\", \"Story analysis\", \"Literary themes\"],\n",
      "  \"low_level_keywords\": [\"Character development\", \"Plot summary\", \"Conflict\", \"Setting\", \"Symbolism\"]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Local query uses 15 entites, 4 relations, 1 text units\n",
      "INFO:lightrag:Global query uses 19 entites, 15 relations, 1 text units\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided entities and relationships, several prominent themes emerge that might define the various narratives referenced, especially in relation to specific works:\n",
      "\n",
      "1. **Cultural Experiences**: The interaction between cultures is notably represented in *Venba*, where themes revolve around the immigrant experience and the significance of food in bridging cultures. Similarly, *Humanity* incorporates aspects of societal interactions in a creative gaming format, showcasing how experiences shape narratives.\n",
      "\n",
      "2. **Systemic Issues**: In *Killers of the Flower Moon*, themes of systemic racism and white supremacy are critically explored. These themes shed light on historic injustices and the continual repercussions faced by marginalized communities, highlighting the importance of historical context in contemporary discussions about race relations.\n",
      "\n",
      "3. **Identity and Independence**: The film *Poor Things* engages with themes of identity, particularly through a reanimated womanâ€™s journey towards self-discovery. This exploration is connected to themes found in *Neo-Frankenstinian fables*, which often grapple with identity and existential questions.\n",
      "\n",
      "4. **Survival and Competition**: Works like *Curve* and *Squid Game* exhibit themes of survival amid extreme challenges, focusing on the psychological impacts of competition and desperation. These narratives examine the lengths to which individuals will go for survival and the moral implications of such competition.\n",
      "\n",
      "5. **Gender and Agency**: The portrayal of female characters, especially in the context of works like *The Legend of Zelda*, sheds light on issues of agency and characterization. Narratives may explore the tension between traditional gender roles and the quest for empowerment within the stories.\n",
      "\n",
      "6. **Historical Reflections**: Both *Rustin* and *Sputnik* incorporate themes related to historical contexts, either by addressing civil rights movements or drawing on the backdrop of the Cold War, respectively. These historical elements shape character motivations and the broader narrative landscape.\n",
      "\n",
      "These themes not only manifest within the individual works but also interact and overlap, providing layers of meaning and insights that reflect contemporary societal issues and human experiences.\n",
      "5715\n"
     ]
    }
   ],
   "source": [
    "context,retrieval_context=rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "print(context)\n",
    "print(count_tokens(retrieval_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>Did Polygon recommend Nintendo Switch games be...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>temporal_query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>After The Independent - Life and Style's repor...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>temporal_query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>Who is the individual facing a criminal trial ...</td>\n",
       "      <td>Sam Bankman-Fried</td>\n",
       "      <td>inference_query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Who is the individual that, despite not being ...</td>\n",
       "      <td>Sam Bankman-Fried</td>\n",
       "      <td>inference_query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>Who is the individual that persuaded Adam Yedi...</td>\n",
       "      <td>Sam Bankman-Fried</td>\n",
       "      <td>inference_query</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query             answer  \\\n",
       "1266  Did Polygon recommend Nintendo Switch games be...                Yes   \n",
       "1749  After The Independent - Life and Style's repor...                Yes   \n",
       "2050  Who is the individual facing a criminal trial ...  Sam Bankman-Fried   \n",
       "393   Who is the individual that, despite not being ...  Sam Bankman-Fried   \n",
       "1544  Who is the individual that persuaded Adam Yedi...  Sam Bankman-Fried   \n",
       "\n",
       "        question_type  \n",
       "1266   temporal_query  \n",
       "1749   temporal_query  \n",
       "2050  inference_query  \n",
       "393   inference_query  \n",
       "1544  inference_query  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "musique_df=pd.read_parquet(r\"D:\\github\\TGRAG_eval\\MultiHop-RAG\\MultiHopRAG_175_sampled.parquet\")\n",
    "musique_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists to store retrieved contexts and responses\n",
    "retrieved_contexts = []\n",
    "responses = []\n",
    "processing_time = []\n",
    "retrieval_tokens = []\n",
    "\n",
    "# Iterate over the first 175 samples of the musique_data dataframe\n",
    "for index in tqdm(range(len(musique_df)), desc=\"Processing samples\"):\n",
    "    question = musique_df.iloc[index]['query']  # Get the question from the dataframe\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Get the answer from TGRAG_search\n",
    "    response, retrieved_context = rag.query(question, param=QueryParam(mode=\"hybrid\",with_retrieval_context=True))\n",
    "\n",
    "    # Calculate processing time\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Append the retrieved context and response to the lists\n",
    "    retrieved_contexts.append(retrieved_context)\n",
    "    responses.append(response)\n",
    "    processing_time.append(elapsed_time)\n",
    "    retrieval_tokens.append(count_tokens(retrieved_context))\n",
    "\n",
    "# Add the retrieved contexts and responses to the musique_data dataframe\n",
    "musique_df['retrieved_context'] = retrieved_contexts\n",
    "musique_df['response'] = responses\n",
    "musique_df['processing_time'] = processing_time\n",
    "musique_df['retrieval_tokens'] = retrieval_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_MultiHopRAG\\output\\LightRAG_MultiHopRAG_175sample_responses2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Average processing time: 6.85 seconds'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_processing_time = musique_df['processing_time'].mean()\n",
    "display(f\"Average processing time: {average_processing_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluations:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 128/175 [05:59<01:27,  1.86s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.421238 seconds\n",
      "Processing evaluations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [07:36<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm  # Ensure tqdm is imported\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def get_gpt4_response(answer, response):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please evaluate if the response matches the reference answer.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Instructions\\nYou will receive a ground truth answer (referred to as Answer) and a model-generated answer (referred to as Response). Your task is to compare the two and determine whether they align.\\n\\nNote: The ground truth answer may sometimes be embedded within the model-generated answer. You need to carefully analyze and discern whether they align.\\nYour Output:\\nIf the two answers align, respond with yes.\\nIf they do not align, respond with no.\\nIf you are very uncertain, respond with unclear.\\nYour response should first include yes, no, or unclear, followed by an explanation.\\n\\nExample 1\\nAnswer: Houston Rockets\\nResponse: The basketball player who was drafted 18th overall in 2001 is Jason Collins, who was selected by the Houston Rockets.\\nExpeted output: yes\\n\\nExample 2\\nAnswer: no\\nResponse: Yes, both Variety and The Advocate are LGBT-interest magazines. The Advocate is explicitly identified as an American LGBT-interest magazine, while Variety, although primarily known for its coverage of the entertainment industry, also addresses topics relevant to the LGBT community.\\n Expected output: no\\n\\nInput Data Format\\nGround Truth Answer: {answer}\\nModel Generated Answer: {response}\\n\\nExpected Output\\nyes, no, or unclear\\nAn explanation of your choice.\\n\\nOutput:\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting GPT-4 response: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "# Wrap the loop with tqdm to monitor progress\n",
    "for idx, row in tqdm(musique_df.iterrows(), total=len(musique_df), desc=\"Processing evaluations\"):\n",
    "    \n",
    "    # Get evaluation from GPT-4\n",
    "    evaluation = get_gpt4_response(row['answer'], row['response'])\n",
    "    \n",
    "    # Update dataframe with evaluation\n",
    "    musique_df.at[idx, 'gpt4_evaluation'] = evaluation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r\"C:\\Users\\Terry_Xu\\Desktop\\LightRAG_MultiHopRAG\\output\\LightRAG_MultiHopRAG_175sample_responses_with_gpt4_evaluation2.parquet\"\n",
    "musique_df.to_parquet(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of evaluations starting with 'yes': 81.14%\n"
     ]
    }
   ],
   "source": [
    "musique_df['evaluation_result'] = musique_df['gpt4_evaluation'].apply(lambda x: 1 if 'yes' in x.lower() else 0)\n",
    "\n",
    "# Calculate the percentage of 1s\n",
    "percentage_yes = (musique_df['evaluation_result'].sum() / len(musique_df)) * 100\n",
    "\n",
    "print(f\"Percentage of evaluations starting with 'yes': {percentage_yes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      175.000000\n",
       "mean      8920.731429\n",
       "std       1289.194911\n",
       "min       5717.000000\n",
       "25%       8231.500000\n",
       "50%       9134.000000\n",
       "75%       9896.500000\n",
       "max      12006.000000\n",
       "Name: retrieval_tokens, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musique_df['retrieval_tokens'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
